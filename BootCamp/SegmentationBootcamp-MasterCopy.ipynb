{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This tutorial will give an example application of using deep learning for medical image segmentation. This example will demonstrate how to train a convolutional neural network for the purpose of lung segmentation in CT images. The tutorial will have 3 main parts:\n",
    "1. Loading and preparing data for model training\n",
    "2. Creating, training, and evaluating a deep learning segmentation model\n",
    "3. Making improvements to the model with skip connections\n",
    "\n",
    "Keep an eye out for questions through this demo to test your new DL knowledge and critical thinking. There are answers at the end of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some modules that we will definitely need throughout this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os # operating system operations \n",
    "import numpy as np # number crunching\n",
    "np.random.seed(1) # set seed for random number generator\n",
    "import keras # our deep learning library\n",
    "import matplotlib.pyplot as plt # for plotting our results\n",
    "# set plotting to be in-line and interactive\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import other necessary modules as we go and need them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Preparation\n",
    "All deep learning applications start with getting the data. In this case, the data has already been collected from subjects through CT scans and annotations have been made. \n",
    "Now that all the data is available, we need to load in this data in an organized way and get it ready to feed into a deep learning model for training.\n",
    "We already have the data downloaded and extracted to the sub-directory 'LCTSC' in your working directory, so we can dive right in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First, let's get all the subject directories. We'll do this by proceeding\n",
    "# through the directory structure and grabbing the ones we want.\n",
    "# We'll use the package glob to make this easy\n",
    "import glob\n",
    "# We know our initial directory: LCTSC. Let's add that to our current\n",
    "# directory to get the full path\n",
    "initial_dir = os.path.join(os.getcwd(),'LCTSC')\n",
    "# Now we'll get all the subject directories using glob\n",
    "subj_dirs = glob.glob(os.path.join(initial_dir,'LCTSC*'))\n",
    "# Now all the subject directories are contained in a list\n",
    "# Let's grab the first one in that list and look for the data\n",
    "cur_dir = subj_dirs[1]\n",
    "# The next directory level just has 1 directory, so we'll grab that\n",
    "cur_dir = glob.glob(os.path.join(cur_dir, \"*\", \"\"))[0]\n",
    "# Now we have the dicom image directory and the label directory\n",
    "# The dicom iamge directory starts with a 0 so we'll find that one first\n",
    "dcm_dir = glob.glob(os.path.join(cur_dir, \"0*\", \"\"))[0]\n",
    "# Let's grab the label directory while we're at it. It starts with a 1\n",
    "lbl_dir = glob.glob(os.path.join(cur_dir, \"1*\", \"\"))[0]\n",
    "# Now, we can get the list of dicom files that we need to \n",
    "# load for this subject\n",
    "# We just have to look for .dcm files in the dcm_dir we found\n",
    "dicom_files = glob.glob(os.path.join(dcm_dir, \"*.dcm\"))\n",
    "# Great. Let's get the label filepath too\n",
    "# It's just contained in a single dicom-rt file in the label directory\n",
    "lbl_file = glob.glob(os.path.join(lbl_dir,\"*.dcm\"))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have all the file paths for this subject. Now we need to actually load in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We'll need the PyDicom package to read the dicoms\n",
    "import pydicom\n",
    "# First, we'll load in all the dicom data to a list\n",
    "dicms = [pydicom.read_file(fn) for fn in dicom_files]\n",
    "# These likely won't be in slice order, so we'll need to sort them\n",
    "# using the ImagePositionPatient header tag\n",
    "dicms.sort(key = lambda x: float(x.ImagePositionPatient[2]))\n",
    "# Then, stack all the pixel data together into a 3D array\n",
    "# We'll convert the data from integers to floats while doing this\n",
    "ims = np.stack([dcm.pixel_array.astype(np.float) for dcm in dicms])\n",
    "# The last thing we will do is normalize all the images\n",
    "# There are a variety of normalization methods used. This one\n",
    "# is pretty common\n",
    "for im in ims:\n",
    "    im -= np.mean(im)\n",
    "    im /= np.std(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1: What other choices could we make for input image normalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in segmentation masks\n",
    "We just loaded the input data for our deep learning model. Now we need to create the target masks that we want to train our model to output.\n",
    "\n",
    "We want our output to be in the form of a binary mask- 1's where the pixels are part of the region of interest, and 0's where they are not.\n",
    "Unfortunately, the data we have came from a segmentation program where it was stored as contours- the boundaries of the region of interest.\n",
    "\n",
    "We will have to load in the contour data and convert it into the masks we need.\n",
    "\n",
    "The data we have actually has contours for the heart, lungs, spine, and esophagus. For the purpose of this example, we will focus on the lungs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's go back and load the label file we already found\n",
    "label = pydicom.read_file(lbl_file)\n",
    "# First, get the contour data.\n",
    "# We need to figure out which contours are the lungs\n",
    "contour_names = [s.ROIName for s in label.StructureSetROISequence]\n",
    "# Get the right and left lung indices\n",
    "r_ind = contour_names.index('Lung_R')\n",
    "l_ind = contour_names.index('Lung_L')\n",
    "# Extract the corresponding contours and combine\n",
    "contour_right = [s.ContourData for s in \n",
    "                 label.ROIContourSequence[r_ind].ContourSequence]\n",
    "contour_left = [s.ContourData for s in \n",
    "                label.ROIContourSequence[l_ind].ContourSequence]\n",
    "contours = contour_left + contour_right\n",
    "# Next, we need to setup the coordinate system for our images\n",
    "# to make sure our contours are aligned\n",
    "# First, the z position\n",
    "z = [d.ImagePositionPatient[2] for d in dicms]\n",
    "# Now the rows and columns\n",
    "# We need both the position of the origin and the\n",
    "# spacing between voxels\n",
    "pos_r = dicms[0].ImagePositionPatient[1]\n",
    "spacing_r = dicms[0].PixelSpacing[1]\n",
    "pos_c = dicms[0].ImagePositionPatient[0]\n",
    "spacing_c = dicms[0].PixelSpacing[0]\n",
    "# Now we are ready to create our mask\n",
    "# First, preallocate an array of zeros\n",
    "mask = np.zeros_like(ims)    \n",
    "# we are going to need a contour-to-mask converter\n",
    "from skimage.draw import polygon\n",
    "# now loop over the different slices that each contour is on\n",
    "for c in contours:\n",
    "    nodes = np.array(c).reshape((-1, 3))\n",
    "    assert np.amax(np.abs(np.diff(nodes[:, 2]))) == 0\n",
    "    zNew = [round(elem,1) for elem in z]\n",
    "    try:\n",
    "        z_index = z.index(nodes[0,2])\n",
    "    except ValueError:\n",
    "        z_index = zNew.index(nodes[0,2])\n",
    "    r = (nodes[:, 1] - pos_r) / spacing_r\n",
    "    c = (nodes[:, 0] - pos_c) / spacing_c\n",
    "    rr, cc = polygon(r, c)\n",
    "    mask[z_index,rr, cc] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have a mask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the pieces we need:\n",
    "* Inputs\n",
    "* Targets\n",
    "\n",
    "...but just for the first subject out of ten. Now we just to repeat for all of the subjects.\n",
    "\n",
    "Luckily, we have provided a pre-made function that does everything we just did in one move. All we have to do is call this function on each of the directories we already collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# function for loading in input and target data given a directory\n",
    "from Demo_Functions import GetLCTSCdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, apply this function to each subject directory.\n",
    "\n",
    "This will take a little bit to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = [GetLCTSCdata(d) for d in subj_dirs]\n",
    "# get all images together as inputs\n",
    "inputs = np.concatenate([d[0] for d in data])\n",
    "# get all masks together as targets\n",
    "targets = np.concatenate([d[1] for d in data])\n",
    "# clear a couple large variables that are no longer needed\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it would be good to check that our data loaded correctly and the masks correspond to the input images. We'll using the python plotting package matplotlib to display a sample image and mask side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "disp_ind = 50\n",
    "plt.figure()\n",
    "disp = np.c_[inputs[disp_ind,...],\n",
    "             targets[disp_ind,...]]\n",
    "plt.imshow(disp,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a couple more pre-processing steps.\n",
    "First, our images are 512x512. That's pretty large for most deep learning applications. It's certainly doable, but for the purpose of this demonstration we will downsample to 256x256 so that the processing is faster.\n",
    "\n",
    "We'll use another scikit-image function for this. It will also take a little bit to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "inputs_rs = np.zeros((inputs.shape[0],256,256))\n",
    "for i,im in enumerate(inputs):\n",
    "    inputs_rs[i] = resize(im,(256,256))\n",
    "targets_rs = np.zeros((targets.shape[0],256,256))\n",
    "for i,im in enumerate(targets):\n",
    "    targets_rs[i] = resize(im,(256,256))\n",
    "    \n",
    "inputs = inputs_rs\n",
    "targets = targets_rs\n",
    "del inputs_rs, targets_rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to add a singleton dimesion to these arrays. This is necessary because the deep learning model we will create will expect our input to have color channels. Since our images are grayscale, they will just have a single \"color\" channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# add a singleton dimension to the input and target arrays\n",
    "inputs = inputs[...,np.newaxis]\n",
    "targets = targets[...,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question: What could be another use for having multiple input channels?\n",
    "Hint: Think MRI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our data is now ready for training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't just need training data. We need a way of determining if our model is overfitting. This means we need both training data and also another set of data to check the performance on data that is not being trained on. \n",
    "\n",
    "We can split some of our data off and use it for validation during the training process. Let's take 10% of the first slices and use them for this purpose. This will be equal to the first subject, so we won't have any overlap of subjects between the different sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the total number of slices\n",
    "num_slices = inputs.shape[0]\n",
    "# Find the cutoff- set to 90% train and 10% validation\n",
    "split_ind = np.int(.1*num_slices)\n",
    "# split into training and validation sets using the cutoff\n",
    "x_val = inputs[:split_ind]\n",
    "y_val = targets[:split_ind]\n",
    "x_train = inputs[split_ind:]\n",
    "y_train = targets[split_ind:]\n",
    "# finally, shuffle the order of the training data\n",
    "# being sure to keep the inputs and targets in the \n",
    "# same order\n",
    "sort_r = np.random.permutation(x_train.shape[0])\n",
    "x_train = np.take(x_train,sort_r,axis=0)\n",
    "y_train = np.take(y_train,sort_r,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOW, our data is ready!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Building a segmentation network\n",
    "\n",
    "We will build a deep convolutional neural network layer by layer, using Keras' high-level libraries that are relatively easy to work with to create exactly the network that we want.\n",
    "\n",
    "For this segmentation problem, the most common and effective networks follow a style known as 'convolutional encoder-decoder' or CED. This means that using convolutional layers we will downsample, or 'encode', our input data, and then upsample, or 'encode' back to our original input size. In this way, the convolutional layers will learn to create a mapping of our input images into a segmentation mask.\n",
    "\n",
    "Let's get to work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need an input layer that takes our inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import Input from keras layers\n",
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input layer just needs the shape of the input we are providing. The shape dimensions are [sample,row,column,channel].\n",
    "\n",
    "For this 2D network, our samples are different slices. We don't need to provide this dimension to the input layer, since we will feed those samples in as batches during training. But we need the rest of the dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create our input layer by giving it an input shape\n",
    "inp = Input(shape=x_train.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will add on convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import 2D convolution layer from keras layers\n",
    "from keras.layers import Conv2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax for adding layers to our network is:\n",
    "\n",
    "`newlayer = LayerType(layer_parameters)(input_layer)`\n",
    "\n",
    "   newlayer: the variable that stores the current output of the network.  \n",
    "   LayerType: the type of the new layer we are adding onto the network, in this case Conv2D layers.  \n",
    "   layer_parameters: the inputs we provide to define the new layer. For Conv2D layers, this is given as (number of filters, size of filters, and type of nonlinearity applied to the layer).  \n",
    "   input_layer: the previous layer that our new layer is going to be connected to.\n",
    "   \n",
    "So for example: `x = Conv2D(10,(3,3), activation='relu')(inp)` creates a 2D convolutional layer with 10 filters that are 3x3 in size. The non-linearity (activation) is a Rectified Linear Unit, and it takes 'inp' as an input and gives its output as x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final note before we build the model. The filters (or 'kernels') are intialized in the background by some random distribution before training. Different distributions can greatly affect how quickly the model learns, or whether it converges at all. Each task can require different intialization distributions and usually requires playing around with different options. For the models we are using today, we already did this work for you and found that the He Normal distribution is most effective (He et al., http://arxiv.org/abs/1502.01852). We will set this parameter in all the convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "init = 'he_normal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without further ado, let's make a convolutional neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make the first convolutional layer with 10 filters, 3x3 filter size,\n",
    "# ReLU activation, and set kernel_initializer to init as defined above\n",
    "x = Conv2D(10,(3,3),activation='relu',kernel_initializer=init)(inp)\n",
    "# Make 2 more convolutional layers connected in order. Increase the filter\n",
    "# number for each\n",
    "x = Conv2D(20,(3,3),activation='relu',kernel_initializer=init)(x)\n",
    "x = Conv2D(30,(3,3),activation='relu',kernel_initializer=init)(x)\n",
    "# now we will use a strided convolution, which downsamples the input\n",
    "# and increases the network's receptive field. This is part of the\n",
    "# encoding side of the network.\n",
    "x = Conv2D(40,(4,4),strides=(2,2),activation='relu',kernel_initializer=init)(x)\n",
    "# repeat that sequence\n",
    "x = Conv2D(50,(3,3),activation='relu',kernel_initializer=init)(x)\n",
    "x = Conv2D(60,(3,3),activation='relu',kernel_initializer=init)(x)\n",
    "x = Conv2D(70,(3,3),activation='relu',kernel_initializer=init)(x)\n",
    "x = Conv2D(80,(4,4),strides=(2,2),activation='relu',kernel_initializer=init)(x)\n",
    "# now, we will reverse the downsampling using Transposed Convolutions, also\n",
    "# incorrectly but commonly called Deconvolution\n",
    "# This is now the decoding side of the network\n",
    "# Import 2D Transpose Convolution from keras layers\n",
    "from keras.layers import Conv2DTranspose\n",
    "# The syntax is identical. However, we need the decoding side of the network to end\n",
    "# up with the same output size as our images. To do that, the first Transpose layer \n",
    "# we will add will have some padding. Simply add the parameter like this:\n",
    "# padding='same'\n",
    "x = Conv2DTranspose(70,(3,3),activation='relu',kernel_initializer=init,padding='same')(x)\n",
    "# Now add another regular layer (no padding) and a strided layer\n",
    "x = Conv2DTranspose(60,(3,3),activation='relu',kernel_initializer=init)(x)\n",
    "x = Conv2DTranspose(50,(4,4),strides=(2,2),activation='relu',kernel_initializer=init)(x)\n",
    "# Repeat that sequence, but no padding this time\n",
    "x = Conv2DTranspose(40,(3,3),activation='relu',kernel_initializer=init)(x)\n",
    "x = Conv2DTranspose(30,(3,3),activation='relu',kernel_initializer=init)(x)\n",
    "x = Conv2DTranspose(20,(4,4),strides=(2,2),activation='relu',kernel_initializer=init)(x)\n",
    "# Just one more transpose layer to get the sizing right\n",
    "x = Conv2DTranspose(10,(3,3),activation='relu',kernel_initializer=init)(x)\n",
    "# Finally, our output layer will need to have a single output\n",
    "# channel corresponding to a single segmentation class\n",
    "# Use sigmoid activation to squash the output to a probability, and\n",
    "# use a 1x1 filter to not change the output size\n",
    "out = Conv2D(1,(1,1),activation='sigmoid',kernel_initializer=init)(x)\n",
    "# Now, we have a graph of layers created but they are not yet a model\n",
    "# Fortunately, Keras makes it easy to make a model out of a graph\n",
    "# just using the input and output layers\n",
    "# Import Model from keras models\n",
    "from keras.models import Model\n",
    "# define our segmentation model using Model\n",
    "SegModel = Model(inp,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a deep learning model created! Let's take a look to make sure we got the image shapes to work out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print a summary of the model we just made\n",
    "SegModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model\n",
    "\n",
    "Compiling the model is the final step before it is ready to train. We need to define our loss function and optimizer that Keras will use to run the training. In this step, Keras will also randomly initialize the weights of our network- so every time the network is trained, it has a different starting point and it is possible to get different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "The Dice coefficient is not only a good segmentation metric, is also works well as a segmentation loss function since it can be converted to being differentiable without much difficulty. Loss functions in Keras need be defined using tensor functions, using the backend API.\n",
    "\n",
    "Here is what that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    dice = (2. * intersection + 1)/(K.sum(y_true_f) + K.sum(y_pred_f) + 1)\n",
    "    # We have calculated dice, but we want to maximize it. \n",
    "    # Keras tries to minimize the loss so we simply return 1- dice\n",
    "    return 1-dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "There are many different optimizers that Keras allows us to use without much trouble. We have provided two examples here and you can try both to see how well they help the model train.\n",
    "\n",
    "Segmentation can be tricky- if you don't have enough data, the model might not converge. We are working with a limited amount of data so that is a possible issue. We have already experimented for you to find parameters that work well for this model. We found that SGD- Stochastic Gradient Descent- works best here. We set a low learning rate and some learning rate decay. We also use Nesterov momentum, which is rarely a bad idea for SGD.\n",
    "The final other parameter we'll use is setting the clipnorm, which means the gradients during training will be clipped to a fixed value. This prevents an issue know as \"exploding gradients\" which causes the model to stop learning.\n",
    "\n",
    "##### Challenge: \n",
    "Experiement with these different settings and see if you can find an optimizer and combination of parameters that gets better results in the same amount of training (epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Setup a SGD optimizer with learning rate of 0.05, decay of 1e-6, momentum of .9\n",
    "# Nesterov momentum enabled, and clipnorm set to 0.5\n",
    "opt = keras.optimizers.SGD(lr=0.05,decay=1e-6,momentum=.9,nesterov=True,clipnorm=0.5)\n",
    "# Compile the segmentation model with Dice as the loss and the created optimizer\n",
    "SegModel.compile(loss=dice_coef,optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "All that's left to do is to \"fit\" the model to our data! \n",
    "\n",
    "Keras takes a few more parameters during model \"fitting\" (training):\n",
    "* Our training data (obviously)\n",
    "* Batch size- how many samples are fed in at once\n",
    "* Epochs- how many times to go through all training data\n",
    "* We ask Keras to constantly report progress (verbose)\n",
    "* Shuffle set to True so the data is in random order for every epoch\n",
    "* Our validation data that will be evaluated at the end of every epoch so we can keep an eye on overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run the model training with our x and y training data, batch size of 32,\n",
    "# 7 epochs, shuffle on, and provide our validation data\n",
    "# Save the output to the variable 'hist'\n",
    "hist = SegModel.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=7,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model\n",
    "After the training is complete, we evaluate the model again on our validation data to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the Dice score from evaluating the model and print it out\n",
    "score = SegModel.evaluate(x_val, y_val, verbose=0)\n",
    "print('Final Dice on validation set: {:.04f}'.format(1-score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to evaluate a model is to look at how both the training and validation losses change during training. Keras gave us this data when we trained the model, now we can plot them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the losses that are stored in the 'hist' variable\n",
    "plt.figure(figsize=(6.0, 4.0));\n",
    "plt.plot(hist.epoch,hist.history['loss'],'b-s')\n",
    "plt.plot(hist.epoch,hist.history['val_loss'],'r-s')\n",
    "plt.legend(['Training Loss',\n",
    "            ' Validation Loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Dice Loss')\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important thing to look for is that the validation loss isn't increasing while the training loss decreases. The divergence of the losses like this means that the model is overfitting- it is getting really good at the training data that it sees, but it is getting worse at the data that it doesn't see. This means the model won't be very helpful when we want to apply it to new data.\n",
    "Due to the random initialization of the network, the exact loss plots will be different every single time you train it. However, for this example, some general statements can be made that probably apply to your results.\n",
    "* The validation and training losses generally go down. This is good- the model is learning.\n",
    "* The validation loss is generally higher than the training loss. This is expected- the model will learn the training data best because that is what it gets direct feedback on. The hope is that it will transfer what it learns to the validation data too.\n",
    "* The validation loss spikes up at some point. This is also pretty normal. The validation data isn't part of the feedback loop so it's not guaranteed that the model will consistently get better results on it. As long as the spikes are isolated and the validation loss follows a general downward trend, it's not anything to worry about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 3: What techniques or strategies can be used to mitigate issues with overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful way to evaluate a model is to just look at the outputs. We can look at a sample image to see how the mask looks compared to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the predictions of the model on the validation inputs\n",
    "predictions = SegModel.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pick a random slice to examine\n",
    "disp_ind = 45\n",
    "# get the CT image, the model predicted mask, and the target mask\n",
    "image = x_val[disp_ind,...,0]\n",
    "predicted_mask = predictions[disp_ind,...,0]\n",
    "truth_mask = y_val[disp_ind,...,0]\n",
    "# normalize image for display\n",
    "image = image-np.min(image)\n",
    "image = image/np.max(image)\n",
    "# create a figure\n",
    "plt.figure()\n",
    "# combine images together into one\n",
    "disp = np.c_[image,predicted_mask,truth_mask]\n",
    "# display image\n",
    "plt.imshow(disp,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results will vary here. It's unlikely that the model already learned a beautiful mask, but hopefully it at least learned something useful and can produce a somewhat reasonable result.\n",
    "\n",
    "Sometimes it helps to get more precise visualization. We have provided a function for viewing the mask on top of the image, so we can maybe start to explain what mistakes the model is making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from Demo_Functions import display_mask\n",
    "\n",
    "display_mask(image,predicted_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 4: Can you explain the errors made by the deep learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a variety of directions to go from here\n",
    "\n",
    "A deeper net gives more representational power to the model. If the problem is too complex for the current network, making it deeper should improve performance.\n",
    "\n",
    "Some mathematical tricks, like batch normalization and ELU activations can help with the learning process and make the model learn quicker.\n",
    "\n",
    "Deep learning models are generally trained for much longer than how long we are running for this example.\n",
    "\n",
    "In segmentation, a particularly useful trick is the use of skip connetions, in which layers from the downsampling part of the network are concatenated with layers on the upsampling part. This both boosts the representational power of the model as well as improves the gradient flow, which also helps the model learn quicker.\n",
    "However, these skip connections take a little bit more effect to implement. Luckily, Keras still makes it pretty easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Functional API\n",
    "So far, we've been making sequential models.\n",
    "Basically, it means that our network\n",
    "has a single, straight path, i.e.\n",
    "\n",
    "![Simple CNN floatchart](https://github.com/jmj23/deep-learning/raw/master/BootCamp/CNN_simple_flowchart.png \"Simple CNN\")\n",
    "\n",
    "Each layer has a single input and output\n",
    "\n",
    "But what if we wanted something more complicated? What if\n",
    "we wanted to implement the skip connections that were just mentioned, for example? Then we would want something like\n",
    "\n",
    "![Connection CNN floatchart](https://github.com/jmj23/deep-learning/raw/master/BootCamp/CNN_connection_flowchart.png \"Connection CNN\")\n",
    "\n",
    "               \n",
    "The extra connection shown is called a skip connection. Skip connections allow the model to consider features that were calculated earlier in the network again, merged with further processed features in practice, this has shown to be hugely helpful in geting precise localization in segmentation outputs.\n",
    "\n",
    "We'll use the same segmentation data so no need to prepare anything new. Let's jump into model creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a segmentation model with skip connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# A new layer we will need for this model\n",
    "from keras.layers import concatenate\n",
    "\n",
    "# start like before\n",
    "inp = Input(shape=x_train.shape[1:])\n",
    "# add on a couple convolutional layers\n",
    "# We don't need to keep track of every layer- just\n",
    "# a few of them. We won't keep track of the first one\n",
    "# but we'll keep the second one and name it x1\n",
    "x = Conv2D(10,kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(inp)\n",
    "x1 = Conv2D(20, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "# We will have to use some specific zero padding\n",
    "# to keep our layer sizes friendly for skip connections\n",
    "# import 2D zero padding from keras layers\n",
    "from keras.layers import ZeroPadding2D\n",
    "# make a zero padding layer that does 1 pad of zeros\n",
    "# on all sides\n",
    "zp = ZeroPadding2D(padding=(1,1))(x1)\n",
    "# Add a strided convolution layer\n",
    "x = Conv2D(30, kernel_size=(4,4),\n",
    "                strides=(2,2),\n",
    "                activation='relu',\n",
    "                kernel_initializer=init)(zp)\n",
    "# Now repeat the process, hanging onto the second layer again\n",
    "x = Conv2D(30, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "x2 = Conv2D(40, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "zp = ZeroPadding2D(padding=(1,1))(x2)\n",
    "x = Conv2D(40, kernel_size=(4,4),\n",
    "                strides=(2,2),\n",
    "                activation='relu',\n",
    "                kernel_initializer=init)(zp)\n",
    "# We've now done 2 downsampling layers, like before.\n",
    "# Now for the decoding side of the network, we will start\n",
    "# adding skip connections\n",
    "# The first couple of layers are the same as usual.\n",
    "x = Conv2D(50, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "x = Conv2D(50, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "# Now our upsampling layer\n",
    "x = Conv2DTranspose(40, kernel_size=(4,4),\n",
    "                        strides=(2,2),\n",
    "                        activation='relu',\n",
    "                        kernel_initializer=init)(x)\n",
    "x = Conv2D(40, kernel_size=(3,3),activation='relu',kernel_initializer=init)(x)\n",
    "# This layer is now the same size as the second layer we kept.\n",
    "# It can be tough to get layers to match up just right in size\n",
    "# Playing around with kernel size and strides is usually needed\n",
    "# so that concatenation can take place. The x,y spatial dimensions\n",
    "# must be the same. Number of channels doesn't matter.\n",
    "# Luckily, we already did the work for you so these layers can be\n",
    "# concatenated\n",
    "x = concatenate([x,x2])\n",
    "# Now continue to add layers for the decoding side of the\n",
    "# network, treating this merged layer like any other\n",
    "x = Conv2D(30, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "x = Conv2D(30, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "x = Conv2DTranspose(20, kernel_size=(4,4),\n",
    "                        strides=(2,2),\n",
    "                        activation='relu',\n",
    "                        kernel_initializer=init)(x)\n",
    "x = Conv2D(20, kernel_size=(3,3),activation='relu',kernel_initializer=init)(x)\n",
    "x = concatenate([x,x1])\n",
    "x = Conv2D(10, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "x = Conv2D(10, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "\n",
    "# Final output layer\n",
    "out = Conv2D(1,kernel_size=(1,1),activation='sigmoid',kernel_initializer=init)(x)\n",
    "\n",
    "# Make the model using the input and output layers\n",
    "# This won't work if the skip connections were not configured correctly\n",
    "SegModel2 = Model(inp,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out a summary of the model to make sure it's what we want.\n",
    "It's a little bit harder to keep track of layers in non-sequential format, but it's still a good way to make sure things look right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the summary of the model\n",
    "SegModel2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, everything else is just like the previous segmentation model. Let's try it out and see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make same optimizer as before and compile the new model\n",
    "opt = keras.optimizers.SGD(lr=0.05,decay=1e-6,momentum=.9,nesterov=True,clipnorm=0.5)\n",
    "SegModel2.compile(loss=dice_coef,optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Running the training with same data, batch size, and epochs as before\n",
    "hist2 = SegModel2.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=7,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the results, including the previous ones\n",
    "# Use different colors for the first and second model\n",
    "plt.figure(figsize=(6.0, 4.0));\n",
    "plt.plot(hist2.epoch,hist2.history['loss'],'r-')\n",
    "plt.plot(hist2.epoch,hist2.history['val_loss'],'r-s')\n",
    "plt.plot(hist.epoch,hist.history['loss'],'b-')\n",
    "plt.plot(hist.epoch,hist.history['val_loss'],'b-s')\n",
    "plt.legend(['Model 2 Training Loss',\n",
    "            'Model 2 Validation Loss',\n",
    "            'Model 1 Training Loss',\n",
    "            'Model 1 Validation Loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Dice Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 5: How can the validation loss be lower than the training loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions of the new model\n",
    "predictions2 = SegModel2.predict(x_val)\n",
    "# display image with mask like before\n",
    "disp_ind = 45\n",
    "image = x_val[disp_ind,...,0]\n",
    "predicted_mask = predictions2[disp_ind,...,0]\n",
    "truth_mask = y_val[disp_ind,...,0]\n",
    "# normalize image for display\n",
    "image = image-np.min(image)\n",
    "image = image/np.max(image)\n",
    "# create a figure\n",
    "plt.figure()\n",
    "# combine images together into one\n",
    "disp = np.c_[image,predicted_mask,truth_mask]\n",
    "# display image\n",
    "plt.imshow(disp,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's better! The network learned much faster, as is apparent in the loss plots. The new model also already has better overall results. Additionally, the mask has more fine detail than the previous version without skip connections. Having these skip connections definitely make a difference. The difference becomes more pronounced for deeper networks (more layers) with more parameters and larger images.\n",
    "\n",
    "To get a complete idea of the performance of the model, we can scroll through the entire validation set and see all the different masks it generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from Demo_Functions import mask_viewer\n",
    "\n",
    "images = np.copy(x_val[...,0])\n",
    "masks = predictions2[...,0]\n",
    "mask_viewer(images,masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know the functional API, you can make any graph you like, train it, and use it! Once you've mastered the syntax and conceptual understanding of how to connect layers, you are only limited by your imagination as far as what kind of network you can build."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Segmentation Example. Happy deep learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to Questions\n",
    "#### 1- What other choices could we make for input image normalization?\n",
    "\n",
    "You can really normalize any way you want. Some other typical methods are normalizing to the ranges [0,1] or [-1,1]. Since these are CT images we could take advanteage of Hounsfield Units and scale water to be 0, air to be -1, etc. The import thing is that the data is fed into the model in a consistent way.\n",
    "    \n",
    "    \n",
    "#### 2- What could be another use for having multiple input channels?\n",
    "\n",
    "In MRI, multiple sequences are usually acquired. It might take some resampling of the data, but you could use multiple sequences as different channels, for example, T1, T2, and 2-point Dixon images. Including more channels in your inputs almost always results in better performance for a deep learning model.\n",
    "\n",
    "#### 3- What techniques or strategies can be used to mitigate issues with overfitting?\n",
    "\n",
    "The best solution is to use more data. That is rarely a possible solution in medical imaging, so there are some alternatives.\n",
    "1. Use data augmentation to synthesize extra data\n",
    "2. Reduce the size or complexity of the network\n",
    "3. Introduce regularization. This can include dropout, batch normalization, or L1/L2 regularization\n",
    "\n",
    "#### 4- Can you explain the errors made by the deep learning model?\n",
    "\n",
    "No! It's really difficult to explain or understand exactly what is going on inside a CNN. There's simply too many parameters involved to be able to pick apart what each one is doing. That's why training always needs validation- it's the only way to check that our model is really learning something useful.\n",
    "\n",
    "#### 5- How can the validation loss be lower than the training loss?\n",
    "\n",
    "It generally isn't, because the model learns from the training data and not the validation data. Only in contrived scenarios could the model actually perform better on the validation data than training. However, sometimes you will see lower validation loss. The explanations could be:\n",
    "* The model has equivalent performance on training and validation, and slight random differences make the validation loss slightly lower\n",
    "* A quirk of Keras. This is how Keras evaluates losses during training:\n",
    "    1. Calculate loss of each training batch during epoch\n",
    "    2. Average these losses together at end of epoch. This is the epoch's training loss\n",
    "    3. Calculate total validation loss at end of epoch.\n",
    "    \n",
    "If a model learns very quickly (frequent in the first few epochs) then the performance of the model at the end of the epoch, when it evaluates the validation data, will better than the average performance during the entire epoch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
