{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "    1. pydicom: conda install -c conda-forge --no-deps pydicom\n",
    "    2. pip install keras\n",
    "    3. conda install scikit-image\n",
    "    4. conda install scipy\n",
    "    5. Demo_functions.py must be on path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This tutorial will give an example application of using deep learning for medical image segmentation. This example will demonstrate how to train a convolutional neural network for the purpose of lung segmentation in CT images. The tutorial will have 3 main parts:\n",
    "1. Loading and preparing data for model training\n",
    "2. Creating, training, and evaluating a deep learning segmentation model\n",
    "3. Making improvements to the model with skip connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some modules that we will definitely need throughout this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os # operating system operations \n",
    "import numpy as np # number crunching\n",
    "import keras # our deep learning library\n",
    "import matplotlib.pyplot as plt # for plotting our results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import other necessary modules as we go and need them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "All deep learning applications start with getting the data. In this case, the data has already been collected from subjects through CT scans and annotations have been made. \n",
    "Now that all the data is available, we need to load in this data in an organized way and get it ready to feed into a deep learning model for training.\n",
    "This guide assumes that you have the data downloaded and extracted to the sub-directory 'LCTSC' in your working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's get all the subject directories. We'll do this by proceeding\n",
    "# through the directory structure and grabbing the ones we want.\n",
    "# We'll use the package glob to make this easy\n",
    "import glob\n",
    "# We know our initial directory: LCTSC. Let's add that to our current\n",
    "# directory to get the full path\n",
    "initial_dir = os.path.join(os.getcwd(),'LCTSC')\n",
    "# Now we'll get all the subject directories using glob\n",
    "subj_dirs = glob.glob(os.path.join(initial_dir,'LCTSC*'))\n",
    "# Now all the subject directories are contained in a list\n",
    "# Let's grab the first one in that list and look for the data\n",
    "cur_dir = subj_dirs[1]\n",
    "# The next directory level just has 1 directory, so we'll grab that\n",
    "cur_dir = glob.glob(os.path.join(cur_dir, \"*\", \"\"))[0]\n",
    "# Now we have the dicom image directory and the label directory\n",
    "# The dicom iamge directory starts with a 0 so we'll find that one first\n",
    "dcm_dir = glob.glob(os.path.join(cur_dir, \"0*\", \"\"))[0]\n",
    "# Let's grab the label directory while we're at it. It starts with a 1\n",
    "lbl_dir = glob.glob(os.path.join(cur_dir, \"1*\", \"\"))[0]\n",
    "# Now, we can get the list of dicom files that we need to \n",
    "# load for this subject\n",
    "# We just have to look for .dcm files in the dcm_dir we found\n",
    "dicom_files = glob.glob(os.path.join(dcm_dir, \"*.dcm\"))\n",
    "# Great. Let's get the label filepath too\n",
    "# It's just contained in a single dicom-rt file in the label directory\n",
    "lbl_file = glob.glob(os.path.join(lbl_dir,\"*.dcm\"))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have all the file paths for this subject. Now we need to actually load in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need the PyDicom package to read the dicoms\n",
    "import pydicom\n",
    "# First, we'll load in all the dicom data to a list\n",
    "dicms = [pydicom.read_file(fn) for fn in dicom_files]\n",
    "# These likely won't be in slice order, so we'll need to sort them\n",
    "# using the ImagePositionPatient header tag\n",
    "dicms.sort(key = lambda x: float(x.ImagePositionPatient[2]))\n",
    "# Then, stack all the pixel data together into a 3D array\n",
    "# We'll convert the data from integers to floats while doing this\n",
    "ims = np.stack([dcm.pixel_array.astype(np.float) for dcm in dicms])\n",
    "# The last thing we will do is normalize all the images to [0,1]\n",
    "# There are a variety of normalization methods used, but\n",
    "# this is simple and seems to work just fine\n",
    "for im in ims:\n",
    "    im /= np.max(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in segmentation masks\n",
    "We just loaded the input data for our deep learning model. Now we need to create the target masks that we want to train our model to output.\n",
    "\n",
    "We want our output to be in the form of a binary mask- 1's where the pixels are part of the region of interest, and 0's where they are not.\n",
    "Unfortunately, the data we have came from a segmentation program where it was stored as contours- the boundaries of the region of interest.\n",
    "\n",
    "We will have to load in the contour data and convert it into the masks we need.\n",
    "\n",
    "The data we have actually has contours for the heart, lungs, spine, and esophagus. For the purpose of this example, we will focus on the lungs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go back and load the label file we already found\n",
    "label = pydicom.read_file(lbl_file)\n",
    "# First, get the contour data.\n",
    "# We need to figure out which contours are the lungs\n",
    "contour_names = [s.ROIName for s in label.StructureSetROISequence]\n",
    "# Get the right and left lung indices\n",
    "r_ind = contour_names.index('Lung_R')\n",
    "l_ind = contour_names.index('Lung_L')\n",
    "# Extract the corresponding contours and combine\n",
    "contour_right = [s.ContourData for s in \n",
    "                 label.ROIContourSequence[r_ind].ContourSequence]\n",
    "contour_left = [s.ContourData for s in \n",
    "                label.ROIContourSequence[l_ind].ContourSequence]\n",
    "contours = contour_left + contour_right\n",
    "# Next, we need to setup the coordinate system for our images\n",
    "# to make sure our contours are aligned\n",
    "# First, the z position\n",
    "z = [d.ImagePositionPatient[2] for d in dicms]\n",
    "# Now the rows and columns\n",
    "# We need both the position of the origin and the\n",
    "# spacing between voxels\n",
    "pos_r = dicms[0].ImagePositionPatient[1]\n",
    "spacing_r = dicms[0].PixelSpacing[1]\n",
    "pos_c = dicms[0].ImagePositionPatient[0]\n",
    "spacing_c = dicms[0].PixelSpacing[0]\n",
    "# Now we are ready to create our mask\n",
    "# First, preallocate an array of zeros\n",
    "mask = np.zeros_like(ims)    \n",
    "# we are going to need a contour-to-mask converter\n",
    "from skimage.draw import polygon\n",
    "# now loop over the different slices that each contour is on\n",
    "for c in contours:\n",
    "    nodes = np.array(c).reshape((-1, 3))\n",
    "    assert np.amax(np.abs(np.diff(nodes[:, 2]))) == 0\n",
    "    zNew = [round(elem,1) for elem in z]\n",
    "    try:\n",
    "        z_index = z.index(nodes[0,2])\n",
    "    except ValueError:\n",
    "        z_index = zNew.index(nodes[0,2])\n",
    "    r = (nodes[:, 1] - pos_r) / spacing_r\n",
    "    c = (nodes[:, 0] - pos_c) / spacing_c\n",
    "    rr, cc = polygon(r, c)\n",
    "    mask[z_index,rr, cc] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have a mask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the pieces we need:\n",
    "* Inputs\n",
    "* Targets\n",
    "\n",
    "...but just for the first subject out of ten. Now we just to repeat for all of the subjects.\n",
    "\n",
    "Luckily, we have provided a pre-made function that does everything we just did in one move. All we have to do is call this function on each of the directories we already collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for loading in input and target data given a directory\n",
    "from Demo_Functions import GetLCTSCdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, apply this function to each subject directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [GetLCTSCdata(d) for d in subj_dirs]\n",
    "# get all images together as inputs\n",
    "inputs = np.concatenate([d[0] for d in data])\n",
    "# get all masks together as targets\n",
    "targets = np.concatenate([d[1] for d in data])\n",
    "# clear a couple large variables that are no longer needed\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a couple more pre-processing steps.\n",
    "First, our images are 512x512. That's pretty large for most deep learning applications. It's certainly doable, but for the purpose of this demonstration we will downsample to 256x256 so that the processing is faster.\n",
    "\n",
    "We'll use another scipy function for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import zoom\n",
    "inputs = zoom(inputs, (1,.5,.5))\n",
    "targets = zoom(targets, (1,.5,.5))\n",
    "targets[targets>.1] = 1\n",
    "targets[targets<.1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to add a singleton dimesion to these arrays. This is necessary because the deep learning model we will create will expect our input to have color channels. Since our images are grayscale, they will just have a single color channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs[...,np.newaxis]\n",
    "targets = targets[...,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our data is now ready for training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't just need training data. We need a way of determining if our model is overfitting. This means we need both training data and also another set of data to check the performance on data that is not being trained on. \n",
    "\n",
    "We can split some of our data off and use it for validation during the training process. Let's take 20% of the last slices and use them for this purpose. This will be equal to the last two subjects, so we won't have any overlap of subjects between the different sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of slices\n",
    "num_slices = inputs.shape[0]\n",
    "# Find the cutoff\n",
    "split_ind = np.int(.8*num_slices)\n",
    "# split into training and validation using common nomenclature\n",
    "x_train = inputs[:split_ind]\n",
    "y_train = targets[:split_ind]\n",
    "x_val = inputs[split_ind:]\n",
    "y_val = targets[split_ind:]\n",
    "# finally, shuffle the order of the training data\n",
    "# being sure to keep the inputs and targets in the \n",
    "# same order...\n",
    "sort_r = np.random.permutation(split_ind)\n",
    "np.take(x_train,sort_r,axis=0,out=x_train)\n",
    "np.take(y_train,sort_r,axis=0,out=y_train)\n",
    "# clear up unneeded variables\n",
    "del inputs,targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOW, our data is ready!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a segmentation network\n",
    "\n",
    "We will build a deep convolutional neural network layer by layer, using Keras' high-level libraries that are relatively easy to work with to create exactly the networkt that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first need an input layer that takes our inputs\n",
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input layer just needs the shape of the input we are providing. The shape dimensions are [sample,row,column,channel].\n",
    "\n",
    "For this 2D network, our samples are different slices. We don't need to provide this dimension to the input layer, since we will feed those samples in as batches during training. But we need the rest of the dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=x_train.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will add on convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax for adding layers to our network is:\n",
    "\n",
    "newlayer = LayerType(layer_parameters)(input_layer)\n",
    "\n",
    "   newlayer: the variable that stores the current output of the network.  \n",
    "   LayerType: the type of the new layer we are adding onto the network, in this case Conv2D layers.  \n",
    "   layer_parameters: the inputs we provide to define the new layer. For Conv2D layers, this is given as (number of filters, size of filters, and type of nonlinearity applied to the layer).  \n",
    "   input_layer: the previous layer that our new layer is going to be connected to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Conv2D(20,(3,3),activation='relu')(inp)\n",
    "# We can reuse the variable 'x' and Keras will remember what the layers\n",
    "# are connected to\n",
    "x = Conv2D(40,(3,3),activation='relu')(x)\n",
    "x = Conv2D(40,(3,3),activation='relu')(x)\n",
    "# now we will use a strided convolution, which downsamples the input\n",
    "# and increases the network's receptive field\n",
    "# We will use zero padding first to make the image shapes\n",
    "# work out correctly\n",
    "from keras.layers import ZeroPadding2D\n",
    "x = ZeroPadding2D(padding=(1,1))(x)\n",
    "x = Conv2D(40,(4,4),strides=(2,2),activation='relu')(x)\n",
    "# repeat that sequence\n",
    "x = Conv2D(60,(3,3),activation='relu')(x)\n",
    "x = Conv2D(80,(3,3),activation='relu')(x)\n",
    "x = Conv2D(80,(3,3),activation='relu')(x)\n",
    "x = ZeroPadding2D(padding=(1,1))(x)\n",
    "x = Conv2D(80,(4,4),strides=(2,2),activation='relu')(x)\n",
    "# now, we will reverse the downsampling using Transposed Convolutions, also\n",
    "# incorrectly but commonly called Deconvolution\n",
    "from keras.layers import Conv2DTranspose\n",
    "x = Conv2DTranspose(60,(4,4),strides=(2,2),activation='relu')(x)\n",
    "x = Conv2DTranspose(40,(3,3),activation='relu')(x)\n",
    "x = Conv2DTranspose(40,(3,3),activation='relu')(x)\n",
    "x = Conv2DTranspose(40,(4,4),strides=(2,2),activation='relu')(x)\n",
    "x = Conv2DTranspose(20,(3,3),activation='relu')(x)\n",
    "x = Conv2DTranspose(20,(3,3),activation='relu')(x)\n",
    "x = Conv2DTranspose(10,(3,3),activation='relu')(x)\n",
    "# finally, our output layer will need to have a single output\n",
    "# channel corresponding to a single segmentation class\n",
    "# We will use sigmoid activation that squashed the output to a probability\n",
    "out = Conv2D(1,(1,1),activation='sigmoid')(x)\n",
    "# Now, we have a graph of layers created but they are not yet a model\n",
    "# Fortunately, Keras makes it easy to make a model out of a graph\n",
    "# just using the input and output layers\n",
    "from keras.models import Model\n",
    "SegModel = Model(inp,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a deep learning model created! Let's take a look to make sure we got the image shapes to work out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 256, 256, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 254, 254, 20)      200       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 252, 252, 40)      7240      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 250, 250, 40)      14440     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 252, 252, 40)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 125, 125, 40)      25640     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 123, 123, 60)      21660     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 121, 121, 80)      43280     \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 119, 119, 80)      57680     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 121, 121, 80)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 59, 59, 80)        102480    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 120, 120, 60)      76860     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 122, 122, 40)      21640     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 124, 124, 40)      14440     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 250, 250, 40)      25640     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 252, 252, 20)      7220      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 254, 254, 20)      3620      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 256, 256, 10)      1810      \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 256, 256, 1)       11        \n",
      "=================================================================\n",
      "Total params: 423,861\n",
      "Trainable params: 423,861\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "SegModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model\n",
    "\n",
    "Compiling the model is the final step before it is ready to train. We need to define our loss function and optimizer that Keras will use to run the training. In this step, Keras will also randomly initialize the weights of our network- so every time the network is trained, it has a different starting point and it is possible to get different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "The Dice coefficient is not only a good segmentation metric, is also works well as a segmentation loss function since it can be converted to being differentiable without much difficulty. Loss functions in Keras need be defined using tensor functions, using the backend API.\n",
    "\n",
    "Here is what that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    dice = (2. * intersection + 1)/(K.sum(y_true_f) + K.sum(y_pred_f) + 1)\n",
    "    # We have calculated dice, but we want to maximize it. \n",
    "    # Keras tries to minimize the loss so we simply return 1- dice\n",
    "    return 1-dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ADAM optimizer is widely used with good performance on the majority of deep learning applications, so we'll use that. The learning rate will be set to a fairly standard value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SegModel.compile(loss=dice_coef,optimizer=keras.optimizers.Adam(lr=1e-4))\n",
    "sgd = keras.optimizers.SGD(lr=0.1, decay=1e-6,momentum=0.9,nesterov=True)\n",
    "SegModel.compile(loss=dice_coef,optimizer=sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "All that's left to do is to \"fit\" the model to our data! We supply our training data, our batch size and epochs that were defined earlier, we ask Keras to constantly report progress (verbose), and we supply some validation data that will be evaluated at the end of every epoch so we can keep an eye on overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1154 samples, validate on 289 samples\n",
      "Epoch 1/16\n",
      "1154/1154 [==============================] - 18s 16ms/step - loss: 0.9237 - val_loss: 0.9137\n",
      "Epoch 2/16\n",
      "1154/1154 [==============================] - 17s 15ms/step - loss: 0.9214 - val_loss: 0.9115\n",
      "Epoch 3/16\n",
      "1154/1154 [==============================] - 17s 15ms/step - loss: 0.9209 - val_loss: 0.9112\n",
      "Epoch 4/16\n",
      "1154/1154 [==============================] - 17s 15ms/step - loss: 0.9208 - val_loss: 0.9110\n",
      "Epoch 5/16\n",
      "1154/1154 [==============================] - 17s 15ms/step - loss: 0.9204 - val_loss: 0.9104\n",
      "Epoch 6/16\n",
      "1154/1154 [==============================] - 17s 15ms/step - loss: 0.9036 - val_loss: 0.8973\n",
      "Epoch 7/16\n",
      "1154/1154 [==============================] - 17s 15ms/step - loss: 0.7594 - val_loss: 0.7144\n",
      "Epoch 8/16\n",
      "1154/1154 [==============================] - 16s 14ms/step - loss: 0.4967 - val_loss: 0.5261\n",
      "Epoch 9/16\n",
      "1154/1154 [==============================] - 16s 14ms/step - loss: 0.8817 - val_loss: 0.9120\n",
      "Epoch 10/16\n",
      " 952/1154 [=======================>......] - ETA: 2s - loss: 0.9234"
     ]
    }
   ],
   "source": [
    "SegModel.fit(x_train, y_train,\n",
    "          batch_size=8,\n",
    "          epochs=16,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model\n",
    "After the training is complete, we evaluate the model again on our validation data to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Dice on validation set: 0.0886211857251\n"
     ]
    }
   ],
   "source": [
    "score = SegModel.evaluate(x_val, y_val, verbose=0)\n",
    "print('Final Dice on validation set:', 1-score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at a sample image to see how the mask looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAACSCAYAAABLwAHLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f61c93c2a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We'll display the prediction and truth next to each other\n",
    "# and see how it faired\n",
    "predictions = SegModel.predict(x_val)\n",
    "# pick a random slice to examine\n",
    "disp_ind = 42\n",
    "plt.figure()\n",
    "disp = np.c_[x_val[disp_ind,...,0],\n",
    "             predictions[disp_ind,...,0],\n",
    "             y_val[disp_ind,...,0]]\n",
    "plt.imshow(disp,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well... it's ok. Not the greatest but it's not terrible either. There are a variety of directions to go from here\n",
    "\n",
    "A deeper net gives more representational power to the model. If the problem is too complex for the current network, making it deeper should improve performance.\n",
    "\n",
    "Some mathematical tricks, like batch normalization and ELU activations can help with the learning process and make the model learn quicker.\n",
    "\n",
    "In segmentation, a particularly useful trick is the use of skip connetions, in which layers from the downsampling part of the network are concatenated with layers on the upsampling part. This both boosts the representational power of the model as well as improves the gradient flow, which also helps the model learn quicker.\n",
    "However, these skip connections take a little bit more effect to implement. Luckily, Keras still makes it pretty easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Functional API\n",
    "So far, we've been making sequential models.\n",
    "Basically, it means that our network\n",
    "has a single, straight path, i.e.\n",
    "\n",
    "![Simple CNN floatchart](https://github.com/jmj23/deep-learning/raw/master/BootCamp/CNN_simple_flowchart.png \"Simple CNN\")\n",
    "\n",
    "Each layer has a single input and output\n",
    "\n",
    "But what if we wanted something more complicated? What if\n",
    "we wanted to implement the skip connections that were just mentioned, for example? Then we would want something like\n",
    "\n",
    "![Connection CNN floatchart](https://github.com/jmj23/deep-learning/raw/master/BootCamp/CNN_connection_flowchart.png \"Connection CNN\")\n",
    "\n",
    "               \n",
    "The extra connection shown is called a skip connection. Skip connections allow the model to consider features that were calculated earlier in the network again, merged with further processed features in practice, this has shown to be hugely helpful in geting precise localization in segmentation outputs.\n",
    "\n",
    "We'll use the same segmentation data so no need to prepare anything new. Let's jump into model creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a segmentation model with skip connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new layer we will need for this model\n",
    "from keras.layers import concatenate\n",
    "\n",
    "# start like before\n",
    "inp = Input(shape=x_train.shape[1:])\n",
    "# add on a couple convolutional layers\n",
    "# We don't need to keep track of every layer- just\n",
    "# a few of them. We won't keep track of the first one\n",
    "# but we'll keep the second one and name it x1\n",
    "x = Conv2D(20,kernel_size=(3,3),padding='same',activation='relu')(inp)\n",
    "x1 = Conv2D(40, kernel_size=(3,3),padding='same',activation='relu')(x)\n",
    "# Add zero padding like before to keep our layer sizes friendly\n",
    "# and then perform downsampling\n",
    "zp = ZeroPadding2D(padding=(1,1))(x1)\n",
    "x = Conv2D(40, kernel_size=(4,4),\n",
    "                 strides=(2,2),\n",
    "                 activation='relu')(zp)\n",
    "# Now repeat the process, hanging onto the second layer again\n",
    "x = Conv2D(60, kernel_size=(3,3),padding='same',activation='relu')(x)\n",
    "x2 = Conv2D(60, kernel_size=(3,3),padding='same',activation='relu')(x)\n",
    "zp = ZeroPadding2D(padding=(1,1))(x2)\n",
    "x = Conv2D(60, kernel_size=(4,4),\n",
    "                strides=(2,2),\n",
    "                activation='relu')(zp)\n",
    "# We've now done 2 downsampling layers, like before.\n",
    "# Now for the decoding side of the network, we will start\n",
    "# adding skip connections\n",
    "# The first couple of layers are the same as usual.\n",
    "x = Conv2D(60, kernel_size=(3,3),padding='same',activation='relu')(x)\n",
    "x = Conv2D(60, kernel_size=(3,3),padding='same',activation='relu')(x)\n",
    "# Now our upsampling layer\n",
    "x = Conv2DTranspose(60, kernel_size=(4,4),\n",
    "                          strides=(2,2),\n",
    "                          activation='relu')(x)\n",
    "x = Conv2D(60, kernel_size=(3,3),activation='relu')(x)\n",
    "# This layer is now the same size as the second layer we kept.\n",
    "# It can be tough to get layers to match up just right in size\n",
    "# Playing around with kernel size and strides is usually needed\n",
    "# so that concatenation can take place. The x,y spatial dimensions\n",
    "# must be the same. Number of channels doesn't matter.\n",
    "# Luckily, we already did the work for you so these layers can be\n",
    "# concatenated\n",
    "x = concatenate([x,x2])\n",
    "# Now continue to add layers for the decoding side of the\n",
    "# network, treating this merged layer like any other\n",
    "x = Conv2D(40, kernel_size=(3,3),padding='same',activation='relu')(x)\n",
    "x = Conv2D(40, kernel_size=(3,3),padding='same',activation='relu')(x)\n",
    "x = Conv2DTranspose(40, kernel_size=(4,4),\n",
    "                          strides=(2,2),\n",
    "                          activation='relu')(x)\n",
    "x = Conv2D(40, kernel_size=(3,3),activation='relu')(x)\n",
    "x = concatenate([x,x1])\n",
    "x = Conv2D(20, kernel_size=(3,3),padding='same',activation='relu')(x)\n",
    "x = Conv2D(20, kernel_size=(3,3),padding='same',activation='relu')(x)\n",
    "\n",
    "# Final output layer\n",
    "out = Conv2D(1,kernel_size=(1,1),activation='sigmoid')(x)\n",
    "\n",
    "SegModel2 = Model(inp,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out a summary of the model to make sure it's what we want.\n",
    "It's a little bit harder to keep track of layers in non-sequential format, but it's still a good way to make sure things look right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SegModel2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, everything else is just like the previous segmentation model. Let's try it out and see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SegModel2.compile(loss=dice_coef,optimizer=sgd)\n",
    "SegModel2.fit(x_train, y_train,\n",
    "          batch_size=16,\n",
    "          epochs=16,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = SegModel2.predict(x_val)\n",
    "plt.figure()\n",
    "disp_ind = 42\n",
    "disp = np.c_[x_val[disp_ind,...,0],\n",
    "             predictions[disp_ind,...,0],\n",
    "             y_val[disp_ind,...,0]]\n",
    "plt.imshow(disp,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well.... ok. It's about the same. However! In the long run (more than 10 epochs), having these skip connections will definitely make a difference. The difference becomes more pronounced for deeper networks (more layers) with more parameters and larger images.\n",
    "\n",
    "Now that you know the functional API, you can make any graph you like, train it, and use it! Once you've mastered the syntax and conceptual understanding of how to connect layers, you are only limited by your imagination as far as what kind of network you can build.\n",
    "\n",
    "Best of luck, and happy deep learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
