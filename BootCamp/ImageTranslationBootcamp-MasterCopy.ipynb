{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This tutorial will give an example application of using deep learning for medical image-to-image translation. This example will demonstrate how to transform a segmentation CNN into a regression CNN for the purpose of predicting T2 images from T1 images. \n",
    "\n",
    "Keep an eye out for questions through this demo to test your new DL knowledge and critical thinking. There are answers at the end of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os # operating system operations \n",
    "import numpy as np # number crunching\n",
    "np.random.seed(1) # set seed for random number generator\n",
    "import keras # our deep learning library\n",
    "import matplotlib.pyplot as plt # for plotting our results\n",
    "# set plotting to be in-line and interactive\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import other necessary modules as we go and need them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "All deep learning applications start with getting the data.\n",
    "\n",
    "We made this one easy for you by compiling the data into an HDF5 file. All we have to do is load all of the inputs and targets and it will be ready to go.\n",
    "\n",
    "First we import the python hdf5 library, h5py. Then we load all the individual datasets in and convert them to Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training, validation, and testing data\n",
    "import h5py\n",
    "with h5py.File('ImageTranslationData.hdf5','r') as hf:\n",
    "    trainX = np.array(hf.get(\"trainX\"))\n",
    "    trainY = np.array(hf.get(\"trainY\"))\n",
    "    valX = np.array(hf.get(\"valX\"))\n",
    "    valY = np.array(hf.get(\"valY\"))\n",
    "    testX = np.array(hf.get(\"testX\"))\n",
    "    testY = np.array(hf.get(\"testY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we will use an ImageDataGenerator so we can augment data on the fly. Before, we used this on a directory to make loading in image data from directories a breeze. However, that only works for classification schemes. For this image to image translation problem, we first had to load all the data into an array. Already did it!\n",
    "\n",
    "Next, we have to setup two generators, one for the input images and one for the target images. Then we will synchronize the generators so they are always creating augmented inputs and targets that match up for training.\n",
    "\n",
    "We have already set these up for you, but it is helpful to look through all the different parameters that are set in the augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # setup image data generator\n",
    "datagen1 = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    shear_range=0.5,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest')\n",
    "datagen2 = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    shear_range=0.5,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "# Provide the same seed and keyword arguments to the fit and flow\n",
    "# in order to synchronize the inputs and targets\n",
    "seed = 1\n",
    "datagen1.fit(trainX, seed=seed)\n",
    "datagen2.fit(trainY, seed=seed)\n",
    "batchsize = 16\n",
    "datagen = zip(datagen1.flow( trainX, None, batchsize, seed=seed),\n",
    "              datagen2.flow( trainY, None, batchsize, seed=seed))\n",
    "\n",
    "# calculate number of epochs and batches\n",
    "numEp = 10\n",
    "steps = np.minimum(np.int(trainX.shape[0]/batchsize*16),1000)\n",
    "numSteps = steps*numEp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all you have to do is call your compiled model on this data generator. Here's the syntax:\n",
    "\n",
    "`hist = Model.fit_generator(DataGenerator, steps_per_epoch,epochs,validation_data=(x,y)`\n",
    "\n",
    "Fill in the syntax with the parameters calculated above, and let it run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RegModel.fit_generator(datagen,\n",
    "                     steps_per_epoch=steps,\n",
    "                     epochs=numEp,\n",
    "                     validation_data=(valX,valY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always a good to check that our data loaded correctly and the inputs correspond to the target images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "disp_ind = 50\n",
    "plt.figure()\n",
    "disp = np.c_[inputs[disp_ind,...,0],\n",
    "             targets[disp_ind,...,0]]\n",
    "plt.imshow(disp,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Segmentation Network into Translation Network\n",
    "\n",
    "We have already built a segmentation CNN in the convolutional encoder-decoder format. This is exactly what we need for image to image translation since our targets are equivalent in size to our inputs.\n",
    "\n",
    "However, there are some changes we need to make. Our output is continuous rather than binary. This means:\n",
    "1. We want a continuous output that isn't squashed by a sigmoid function\n",
    "2. We need a loss function that works with regression error, not overlap error like Dice\n",
    "\n",
    "We will start with the previously made network that has skip connections and had good performance on the segmentation task. \n",
    "\n",
    "Let's get to work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already imported keras, so we don't technically need to import anything. It keep code a lot cleaner to individually import layers, so we'll do that again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import keras layers\n",
    "from keras.layers import Input,Conv2D,ZeroPadding2D\n",
    "from keras.layers import concatenate,Conv2DTranspose\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code used to define the segmentation network in the previous example, including the use of skip connections. Make the appropriate alterations to this code as described above to transform it into a image-to-image translation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=x_train.shape[1:])\n",
    "x = Conv2D(10,kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(inp)\n",
    "x1 = Conv2D(20, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "zp = ZeroPadding2D(padding=(1,1))(x1)\n",
    "x = Conv2D(30, kernel_size=(4,4),\n",
    "                strides=(2,2),\n",
    "                activation='relu',\n",
    "                kernel_initializer=init)(zp)\n",
    "x = Conv2D(30, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "x2 = Conv2D(40, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "zp = ZeroPadding2D(padding=(1,1))(x2)\n",
    "x = Conv2D(40, kernel_size=(4,4),\n",
    "                strides=(2,2),\n",
    "                activation='relu',\n",
    "                kernel_initializer=init)(zp)\n",
    "x = Conv2D(50, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "x = Conv2D(50, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "x = Conv2DTranspose(40, kernel_size=(4,4),\n",
    "                        strides=(2,2),\n",
    "                        activation='relu',\n",
    "                        kernel_initializer=init)(x)\n",
    "x = Conv2D(40, kernel_size=(3,3),activation='relu',kernel_initializer=init)(x)\n",
    "x = concatenate([x,x2])\n",
    "x = Conv2D(30, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "x = Conv2D(30, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "x = Conv2DTranspose(20, kernel_size=(4,4),\n",
    "                        strides=(2,2),\n",
    "                        activation='relu',\n",
    "                        kernel_initializer=init)(x)\n",
    "x = Conv2D(20, kernel_size=(3,3),activation='relu',kernel_initializer=init)(x)\n",
    "x = concatenate([x,x1])\n",
    "x = Conv2D(10, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "x = Conv2D(10, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "\n",
    "# Final output layer\n",
    "out = Conv2D(1,kernel_size=(1,1),activation='linear',kernel_initializer=init)(x)\n",
    "\n",
    "RegModel = Model(inp,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the loss function you wish to use for this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = keras.losses.mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, add an optimizer and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "SegModel.compile(loss=loss,optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "Let's try training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run the model training with our x and y training data, batch size of 16,\n",
    "# 5 epochs, shuffle on, and provide our validation data\n",
    "# Save the output to the variable 'hist'\n",
    "hist = RegModel.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model\n",
    "After the training is complete, we evaluate the model again on our validation data to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the Dice score from evaluating the model and print it out\n",
    "score = RegModel.evaluate(x_val, y_val, verbose=0)\n",
    "print('Final MAE on validation set: {:.04e}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss curves too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the losses that are stored in the 'hist' variable\n",
    "plt.figure(figsize=(6.0, 4.0));\n",
    "plt.plot(hist.epoch,hist.history['loss'],'b-s')\n",
    "plt.plot(hist.epoch,hist.history['val_loss'],'r-s')\n",
    "plt.legend(['Training Loss',\n",
    "            ' Validation Loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Dice Loss')\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important thing to look for is that the validation loss isn't increasing while the training loss decreases. The divergence of the losses like this means that the model is overfitting- it is getting really good at the training data that it sees, but it is getting worse at the data that it doesn't see. This means the model won't be very helpful when we want to apply it to new data.\n",
    "Due to the random initialization of the network, the exact loss plots will be different every single time you train it. However, for this example, some general statements can be made that probably apply to your results.\n",
    "* The validation and training losses generally go down. This is good- the model is learning.\n",
    "* The validation loss is generally higher than the training loss. This is expected- the model will learn the training data best because that is what it gets direct feedback on. The hope is that it will transfer what it learns to the validation data too.\n",
    "* The validation loss spikes up at some point. This is also pretty normal. The validation data isn't part of the feedback loop so it's not guaranteed that the model will consistently get better results on it. As long as the spikes are isolated and the validation loss follows a general downward trend, it's not anything to worry about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 3: What techniques or strategies can be used to mitigate issues with overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful way to evaluate a model is to just look at the outputs. We can look at a sample image to see how the mask looks compared to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the predictions of the model on the validation inputs\n",
    "predictions = SegModel.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pick a random slice to examine\n",
    "disp_ind = 45\n",
    "# get the CT image, the model predicted mask, and the target mask\n",
    "image = x_val[disp_ind,...,0]\n",
    "predicted_mask = predictions[disp_ind,...,0]\n",
    "truth_mask = y_val[disp_ind,...,0]\n",
    "# normalize image for display\n",
    "image = image-np.min(image)\n",
    "image = image/np.max(image)\n",
    "# create a figure\n",
    "plt.figure()\n",
    "# combine images together into one\n",
    "disp = np.c_[image,predicted_mask,truth_mask]\n",
    "# display image\n",
    "plt.imshow(disp,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results will vary here. It's unlikely that the model already learned a beautiful mask, but hopefully it at least learned something useful and can produce a somewhat reasonable result.\n",
    "\n",
    "Sometimes it helps to get more precise visualization. We have provided a function for viewing the mask on top of the image, so we can maybe start to explain what mistakes the model is making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from Demo_Functions import display_mask\n",
    "\n",
    "display_mask(image,predicted_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 4: Can you explain the errors made by the deep learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a variety of directions to go from here\n",
    "\n",
    "A deeper net gives more representational power to the model. If the problem is too complex for the current network, making it deeper should improve performance.\n",
    "\n",
    "Some mathematical tricks, like batch normalization and ELU activations can help with the learning process and make the model learn quicker.\n",
    "\n",
    "Deep learning models are generally trained for much longer than how long we are running for this example.\n",
    "\n",
    "In segmentation, a particularly useful trick is the use of skip connetions, in which layers from the downsampling part of the network are concatenated with layers on the upsampling part. This both boosts the representational power of the model as well as improves the gradient flow, which also helps the model learn quicker.\n",
    "However, these skip connections take a little bit more effect to implement. Luckily, Keras still makes it pretty easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Functional API\n",
    "So far, we've been making sequential models.\n",
    "Basically, it means that our network\n",
    "has a single, straight path, i.e.\n",
    "\n",
    "![Simple CNN floatchart](https://github.com/jmj23/deep-learning/raw/master/BootCamp/CNN_simple_flowchart.png \"Simple CNN\")\n",
    "\n",
    "Each layer has a single input and output\n",
    "\n",
    "But what if we wanted something more complicated? What if\n",
    "we wanted to implement the skip connections that were just mentioned, for example? Then we would want something like\n",
    "\n",
    "![Connection CNN floatchart](https://github.com/jmj23/deep-learning/raw/master/BootCamp/CNN_connection_flowchart.png \"Connection CNN\")\n",
    "\n",
    "               \n",
    "The extra connection shown is called a skip connection. Skip connections allow the model to consider features that were calculated earlier in the network again, merged with further processed features in practice, this has shown to be hugely helpful in geting precise localization in segmentation outputs.\n",
    "\n",
    "We'll use the same segmentation data so no need to prepare anything new. Let's jump into model creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a segmentation model with skip connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# A new layer we will need for this model\n",
    "from keras.layers import concatenate\n",
    "\n",
    "# start like before\n",
    "inp = Input(shape=x_train.shape[1:])\n",
    "# add on a couple convolutional layers\n",
    "# We don't need to keep track of every layer- just\n",
    "# a few of them. We won't keep track of the first one\n",
    "# but we'll keep the second one and name it x1\n",
    "x = Conv2D(10,kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(inp)\n",
    "x1 = Conv2D(20, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "# We will have to use some specific zero padding\n",
    "# to keep our layer sizes friendly for skip connections\n",
    "# import 2D zero padding from keras layers\n",
    "from keras.layers import ZeroPadding2D\n",
    "# make a zero padding layer that does 1 pad of zeros\n",
    "# on all sides\n",
    "zp = ZeroPadding2D(padding=(1,1))(x1)\n",
    "# Add a strided convolution layer\n",
    "x = Conv2D(30, kernel_size=(4,4),\n",
    "                strides=(2,2),\n",
    "                activation='relu',\n",
    "                kernel_initializer=init)(zp)\n",
    "# Now repeat the process, hanging onto the second layer again\n",
    "x = Conv2D(30, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "x2 = Conv2D(40, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "zp = ZeroPadding2D(padding=(1,1))(x2)\n",
    "x = Conv2D(40, kernel_size=(4,4),\n",
    "                strides=(2,2),\n",
    "                activation='relu',\n",
    "                kernel_initializer=init)(zp)\n",
    "# We've now done 2 downsampling layers, like before.\n",
    "# Now for the decoding side of the network, we will start\n",
    "# adding skip connections\n",
    "# The first couple of layers are the same as usual.\n",
    "x = Conv2D(50, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "x = Conv2D(50, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "# Now our upsampling layer\n",
    "x = Conv2DTranspose(40, kernel_size=(4,4),\n",
    "                        strides=(2,2),\n",
    "                        activation='relu',\n",
    "                        kernel_initializer=init)(x)\n",
    "x = Conv2D(40, kernel_size=(3,3),activation='relu',kernel_initializer=init)(x)\n",
    "# This layer is now the same size as the second layer we kept.\n",
    "# It can be tough to get layers to match up just right in size\n",
    "# Playing around with kernel size and strides is usually needed\n",
    "# so that concatenation can take place. The x,y spatial dimensions\n",
    "# must be the same. Number of channels doesn't matter.\n",
    "# Luckily, we already did the work for you so these layers can be\n",
    "# concatenated\n",
    "x = concatenate([x,x2])\n",
    "# Now continue to add layers for the decoding side of the\n",
    "# network, treating this merged layer like any other\n",
    "x = Conv2D(30, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "x = Conv2D(30, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "x = Conv2DTranspose(20, kernel_size=(4,4),\n",
    "                        strides=(2,2),\n",
    "                        activation='relu',\n",
    "                        kernel_initializer=init)(x)\n",
    "x = Conv2D(20, kernel_size=(3,3),activation='relu',kernel_initializer=init)(x)\n",
    "x = concatenate([x,x1])\n",
    "x = Conv2D(10, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "x = Conv2D(10, kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=init)(x)\n",
    "\n",
    "# Final output layer\n",
    "out = Conv2D(1,kernel_size=(1,1),activation='sigmoid',kernel_initializer=init)(x)\n",
    "\n",
    "# Make the model using the input and output layers\n",
    "# This won't work if the skip connections were not configured correctly\n",
    "SegModel2 = Model(inp,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out a summary of the model to make sure it's what we want.\n",
    "It's a little bit harder to keep track of layers in non-sequential format, but it's still a good way to make sure things look right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the summary of the model\n",
    "SegModel2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, everything else is just like the previous segmentation model. Let's try it out and see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make same optimizer as before and compile the new model\n",
    "opt = keras.optimizers.SGD(lr=0.05,decay=1e-6,momentum=.9,nesterov=True,clipnorm=0.5)\n",
    "SegModel2.compile(loss=dice_coef,optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Running the training with same data, batch size, and epochs as before\n",
    "hist2 = SegModel2.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=7,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the results, including the previous ones\n",
    "# Use different colors for the first and second model\n",
    "plt.figure(figsize=(6.0, 4.0));\n",
    "plt.plot(hist2.epoch,hist2.history['loss'],'r-')\n",
    "plt.plot(hist2.epoch,hist2.history['val_loss'],'r-s')\n",
    "plt.plot(hist.epoch,hist.history['loss'],'b-')\n",
    "plt.plot(hist.epoch,hist.history['val_loss'],'b-s')\n",
    "plt.legend(['Model 2 Training Loss',\n",
    "            'Model 2 Validation Loss',\n",
    "            'Model 1 Training Loss',\n",
    "            'Model 1 Validation Loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Dice Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 5: How can the validation loss be lower than the training loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions of the new model\n",
    "predictions2 = SegModel2.predict(x_val)\n",
    "# display image with mask like before\n",
    "disp_ind = 45\n",
    "image = x_val[disp_ind,...,0]\n",
    "predicted_mask = predictions2[disp_ind,...,0]\n",
    "truth_mask = y_val[disp_ind,...,0]\n",
    "# normalize image for display\n",
    "image = image-np.min(image)\n",
    "image = image/np.max(image)\n",
    "# create a figure\n",
    "plt.figure()\n",
    "# combine images together into one\n",
    "disp = np.c_[image,predicted_mask,truth_mask]\n",
    "# display image\n",
    "plt.imshow(disp,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's better! The network learned much faster, as is apparent in the loss plots. The new model also already has better overall results. Additionally, the mask has more fine detail than the previous version without skip connections. Having these skip connections definitely make a difference. The difference becomes more pronounced for deeper networks (more layers) with more parameters and larger images.\n",
    "\n",
    "To get a complete idea of the performance of the model, we can scroll through the entire validation set and see all the different masks it generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from Demo_Functions import mask_viewer\n",
    "\n",
    "images = np.copy(x_val[...,0])\n",
    "masks = predictions2[...,0]\n",
    "mask_viewer(images,masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know the functional API, you can make any graph you like, train it, and use it! Once you've mastered the syntax and conceptual understanding of how to connect layers, you are only limited by your imagination as far as what kind of network you can build."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Segmentation Example. Happy deep learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to Questions\n",
    "#### 1- What other choices could we make for input image normalization?\n",
    "\n",
    "You can really normalize any way you want. Some other typical methods are normalizing to the ranges [0,1] or [-1,1]. Since these are CT images we could take advanteage of Hounsfield Units and scale water to be 0, air to be -1, etc. The import thing is that the data is fed into the model in a consistent way.\n",
    "    \n",
    "    \n",
    "#### 2- What could be another use for having multiple input channels?\n",
    "\n",
    "In MRI, multiple sequences are usually acquired. It might take some resampling of the data, but you could use multiple sequences as different channels, for example, T1, T2, and 2-point Dixon images. Including more channels in your inputs almost always results in better performance for a deep learning model.\n",
    "\n",
    "#### 3- What techniques or strategies can be used to mitigate issues with overfitting?\n",
    "\n",
    "The best solution is to use more data. That is rarely a possible solution in medical imaging, so there are some alternatives.\n",
    "1. Use data augmentation to synthesize extra data\n",
    "2. Reduce the size or complexity of the network\n",
    "3. Introduce regularization. This can include dropout, batch normalization, or L1/L2 regularization\n",
    "\n",
    "#### 4- Can you explain the errors made by the deep learning model?\n",
    "\n",
    "No! It's really difficult to explain or understand exactly what is going on inside a CNN. There's simply too many parameters involved to be able to pick apart what each one is doing. That's why training always needs validation- it's the only way to check that our model is really learning something useful.\n",
    "\n",
    "#### 5- How can the validation loss be lower than the training loss?\n",
    "\n",
    "It generally isn't, because the model learns from the training data and not the validation data. Only in contrived scenarios could the model actually perform better on the validation data than training. However, sometimes you will see lower validation loss. The explanations could be:\n",
    "* The model has equivalent performance on training and validation, and slight random differences make the validation loss slightly lower\n",
    "* A quirk of Keras. This is how Keras evaluates losses during training:\n",
    "    1. Calculate loss of each training batch during epoch\n",
    "    2. Average these losses together at end of epoch. This is the epoch's training loss\n",
    "    3. Calculate total validation loss at end of epoch.\n",
    "    \n",
    "If a model learns very quickly (frequent in the first few epochs) then the performance of the model at the end of the epoch, when it evaluates the validation data, will better than the average performance during the entire epoch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
